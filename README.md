
# Apache Airflow ETL Project

This is a beginner-friendly ETL pipeline project using **Apache Airflow** with **Docker Compose**. It reads a CSV file, performs simple transformations using pandas, and writes the output to another CSV file.

## ğŸ› ï¸ Project Stack

- **Apache Airflow** (via Docker Compose)
- **Python**
- **pandas**
- **Postgres** (as Airflow metadata DB)
- **Redis** (for scheduler/message queue)

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ dags/                  # Airflow DAGs
â”‚   â””â”€â”€ etl_csv_pipeline.py
â”œâ”€â”€ data/                  # Input and output CSVs
â”‚   â”œâ”€â”€ raw/customers.csv
â”‚   â””â”€â”€ processed/processed_customers.csv
â”œâ”€â”€ docker-compose.yaml    # Airflow setup
â”œâ”€â”€ .env                   # Airflow environment config
â”œâ”€â”€ logs/                  # Airflow logs
â””â”€â”€ plugins/               # (empty, for future custom plugins)
```

## â–¶ï¸ How to Run

1. Clone the repo and navigate to the directory:

   ```bash
   git clone https://github.com/yourusername/airflow_etl_project.git
   cd airflow_etl_project
   ```

2. Start the Airflow containers:

   ```bash
   docker compose up airflow-init
   docker compose up
   ```

3. Visit [http://localhost:8080](http://localhost:8080)  
   Login with:
   - **Username**: `airflow`
   - **Password**: `airflow`

4. Trigger the DAG `etl_csv_pipeline` to run the ETL job.

## âœ… Output

After successful DAG execution:
- Youâ€™ll find a `processed_customers.csv` file with cleaned and deduplicated customer records in `data/processed`.

## ğŸ“„ License

This project is open-source and free to use.
